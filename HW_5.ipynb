{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d0f978-9062-44e5-b42b-c7b6d5ca9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d8e35-83e4-467d-af10-6205b1bbf334",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4768b00-dd05-47ea-93b1-c4249866ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 08:16:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[4]\")\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e3f51c-6aff-4bfc-ba87-ef7f32016413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3dfdd-1e1d-48e9-a0f4-d9eefaf4f4cb",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 87MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f898d2e-6392-4b02-a3ef-9de521f3de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'fhv_tripdata_2019-10.csv.gz'\n",
    "file_url = f'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{file_name}'\n",
    "spark.sparkContext.addFile(file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931a9ed-1333-4231-af87-a230ba847746",
   "metadata": {},
   "source": [
    "### schema inference with pandas trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bf4567-c1cf-4065-9a17-f3c7313e9b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the first 100 rows using Pandas\n",
    "first_100_rows = pd.read_csv(file_url, compression='gzip', nrows=100, parse_dates=['pickup_datetime', 'dropOff_datetime'])\n",
    "first_100_rows_sp = spark.createDataFrame(first_100_rows)\n",
    "first_100_rows_sp = (first_100_rows_sp\n",
    "                     .withColumn(\"SR_Flag\", first_100_rows_sp[\"SR_Flag\"].cast(IntegerType()))\n",
    "                     .withColumn(\"DOLocationID\", first_100_rows_sp[\"DOLocationID\"].cast(IntegerType()))\n",
    "                     .withColumn(\"PULocationID\", first_100_rows_sp[\"PULocationID\"].cast(IntegerType())))\n",
    "first_100_rows_sp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0881c635-3d5f-4eb6-b9f8-455dc47a6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(SparkFiles.get(file_name), header=True, schema=first_100_rows_sp.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98f79-0146-445c-8beb-a17976b7fc5d",
   "metadata": {},
   "source": [
    "### Write the obtained csv to partitioned parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5502a06-1516-4c2f-861b-de5fbc659a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(6).write.parquet('fhv/2019/10/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba2e156-81c7-4b64-8b73-420c4c06a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 83896\n",
      "-rw-r--r--@ 1 alexlitvinov  staff     0B Mar  6 08:16 _SUCCESS\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.4M Mar  6 08:16 part-00000-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.3M Mar  6 08:16 part-00001-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.4M Mar  6 08:16 part-00002-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.4M Mar  6 08:16 part-00003-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.4M Mar  6 08:16 part-00004-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   6.4M Mar  6 08:16 part-00005-3fe45406-900f-4d51-903d-2e2f4891ca8c-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhv/2019/10/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d467b9-426c-446a-a738-ef34c3363a1f",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "**Count records** \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 108,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 62,610\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Be aware of columns order when defining schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f36788d-2ac8-444a-b3b0-ee1c0d91e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5995ae-996e-4a77-9693-b92979d96e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(1)\n",
    "from fhv\n",
    "where DATE(pickup_datetime) = '2019-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08d3a5-ba35-4e94-b103-bb0c31ce6978",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 631,152.50 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f65a804-6a1e-4b35-af35-43361b3d7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max_diff_in_hours|\n",
      "+-----------------+\n",
      "|         631152.5|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT max((unix_timestamp(dropOff_datetime) - unix_timestamp(pickup_datetime))/3600) as max_diff_in_hours\n",
    "FROM fhv\n",
    "ORDER BY 1 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf6115-ddac-48bc-80fd-f010551fc03c",
   "metadata": {},
   "source": [
    "\n",
    "### Question 5: \n",
    "\n",
    "**User Interface**\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "- 80\n",
    "- 443\n",
    "- 4040\n",
    "- 8080\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf60834-b276-4974-ab04-1cb5e927167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI port: 4040\n"
     ]
    }
   ],
   "source": [
    "spark_ui_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "\n",
    "print(\"Spark UI port:\", spark_ui_port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245e2fe-5abd-48a0-a2a3-7d8806594635",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>\n",
    "\n",
    "- East Chelsea\n",
    "- Jamaica Bay\n",
    "- Union Sq\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51ed113-c76c-44f7-a691-1d9922a73501",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv'\n",
    "spark.sparkContext.addFile(zones_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b1db97-ff29-4eb9-8b71-b6d65b1ba58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.csv(SparkFiles.get('taxi_zone_lookup.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14cb7aa-ebff-481f-89ae-a13ea32906ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "630173f3-b10f-4044-9634-dfc3224fcb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                Zone|desc|\n",
      "+--------------------+----+\n",
      "|         Jamaica Bay|   1|\n",
      "|Governor's Island...|   2|\n",
      "| Green-Wood Cemetery|   5|\n",
      "|       Broad Channel|   8|\n",
      "|     Highbridge Park|  14|\n",
      "|        Battery Park|  15|\n",
      "|Saint Michaels Ce...|  23|\n",
      "|Breezy Point/Fort...|  25|\n",
      "|Marine Park/Floyd...|  26|\n",
      "|        Astoria Park|  29|\n",
      "|    Inwood Hill Park|  39|\n",
      "|       Willets Point|  47|\n",
      "|Forest Park/Highl...|  53|\n",
      "|  Brooklyn Navy Yard|  57|\n",
      "|        Crotona Park|  62|\n",
      "|        Country Club|  77|\n",
      "|     Freshkills Park|  89|\n",
      "|       Prospect Park|  98|\n",
      "|     Columbia Street| 105|\n",
      "|  South Williamsburg| 110|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select zones.Zone, count(1) desc\n",
    "from fhv\n",
    "join zones on zones.LocationID = fhv.PULocationID\n",
    "group by zones.Zone\n",
    "order by 2 asc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e901ecc-81ba-416d-8ccd-a4553dba2479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
