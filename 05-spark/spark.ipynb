{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 06:42:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[4]\")\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘taxi+_zone_lookup.csv’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv(\"taxi+_zone_lookup.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_zones.write.parquet('zones', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read FHVHV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "file_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz'\n",
    "spark.sparkContext.addFile(file_url)\n",
    "\n",
    "df_fhvhv = spark.read.csv(SparkFiles.get('fhvhv_tripdata_2021-01.csv.gz'), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer schema by reading first 100 rows into pandas and then converting to spark making small adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first 100 rows using Pandas\n",
    "first_100_rows = pd.read_csv(file_url, compression='gzip', nrows=100, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "SR_Flag                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_100_rows.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_100_rows_sp = spark.createDataFrame(first_100_rows)\n",
    "first_100_rows_sp = (first_100_rows_sp\n",
    "                     .withColumn(\"SR_Flag\", first_100_rows_sp[\"SR_Flag\"].cast(IntegerType()))\n",
    "                     .withColumn(\"DOLocationID\", first_100_rows_sp[\"DOLocationID\"].cast(IntegerType()))\n",
    "                     .withColumn(\"PULocationID\", first_100_rows_sp[\"PULocationID\"].cast(IntegerType())))\n",
    "first_100_rows_sp.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read FHVHV data again providing the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.csv(SparkFiles.get('fhvhv_tripdata_2021-01.csv.gz'), header=True, schema=first_100_rows_sp.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the obtained csv to partitioned parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhvhv.repartition(24).write.parquet('fhvhv/2021/01/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that resulting parquet files are considerably smaller than the original csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 461272\n",
      "-rw-r--r--@ 1 alexlitvinov  staff     0B Mar  6 07:08 _SUCCESS\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00000-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00001-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00002-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00003-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00004-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00005-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00006-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Mar  6 07:08 part-00007-3701bd1c-87e1-4837-b98f-d44d1e7a0372-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/01/ | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read repartitioned df back into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-12 09:46:38|2021-01-12 09:59:31|           4|         137|\n",
      "|2021-01-22 18:07:54|2021-01-22 18:13:25|         242|          78|\n",
      "|2021-01-19 08:55:37|2021-01-19 09:09:36|         244|         128|\n",
      "|2021-01-12 04:16:21|2021-01-12 04:37:52|         230|         225|\n",
      "|2021-01-20 17:00:35|2021-01-20 17:12:54|         228|          26|\n",
      "|2021-01-29 17:27:39|2021-01-29 18:03:28|         123|         130|\n",
      "|2021-01-16 14:47:42|2021-01-16 15:06:44|          47|         119|\n",
      "|2021-01-03 05:33:48|2021-01-03 06:04:01|          14|          75|\n",
      "|2021-01-10 16:31:19|2021-01-10 16:36:52|         150|         150|\n",
      "|2021-01-30 17:44:41|2021-01-30 17:52:11|         205|         205|\n",
      "|2021-01-15 15:15:26|2021-01-15 15:40:27|         129|         145|\n",
      "|2021-01-14 09:18:22|2021-01-14 09:45:49|         157|         236|\n",
      "|2021-01-22 15:52:23|2021-01-22 16:01:16|          41|          42|\n",
      "|2021-01-29 04:39:53|2021-01-29 04:49:37|         213|         250|\n",
      "|2021-01-31 02:04:49|2021-01-31 02:25:57|         167|         243|\n",
      "|2021-01-11 20:59:47|2021-01-11 21:12:33|         164|         226|\n",
      "|2021-01-05 16:20:14|2021-01-05 16:29:35|         239|         238|\n",
      "|2021-01-13 12:25:56|2021-01-13 12:28:41|          42|          42|\n",
      "|2021-01-12 22:14:28|2021-01-12 22:26:10|         197|         135|\n",
      "|2021-01-29 14:46:57|2021-01-29 15:14:45|          79|          74|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_fhvhv.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\n",
    " .filter(df_fhvhv.hvfhs_license_num == 'HV0003')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark in-built sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|base_id|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-25 18:08:34|2021-01-25 18:20:34|         247|         169|   NULL| 2021-01-25|  2021-01-25|  e/9ce|\n",
      "|           HV0005|              B02510|2021-01-01 06:52:41|2021-01-01 06:59:34|          14|         227|   NULL| 2021-01-01|  2021-01-01|  e/9ce|\n",
      "|           HV0005|              B02510|2021-01-29 10:47:17|2021-01-29 11:06:12|         134|         138|   NULL| 2021-01-29|  2021-01-29|  e/9ce|\n",
      "|           HV0003|              B02866|2021-01-12 09:46:38|2021-01-12 09:59:31|           4|         137|   NULL| 2021-01-12|  2021-01-12|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-26 22:19:36|2021-01-26 22:43:02|           7|         239|   NULL| 2021-01-26|  2021-01-26|  e/9ce|\n",
      "|           HV0003|              B02866|2021-01-22 18:07:54|2021-01-22 18:13:25|         242|          78|   NULL| 2021-01-22|  2021-01-22|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-23 18:18:33|2021-01-23 18:29:34|          80|         148|   NULL| 2021-01-23|  2021-01-23|  e/9ce|\n",
      "|           HV0003|              B02764|2021-01-19 08:55:37|2021-01-19 09:09:36|         244|         128|   NULL| 2021-01-19|  2021-01-19|  e/acc|\n",
      "|           HV0003|              B02872|2021-01-12 04:16:21|2021-01-12 04:37:52|         230|         225|   NULL| 2021-01-12|  2021-01-12|  e/b38|\n",
      "|           HV0003|              B02884|2021-01-20 17:00:35|2021-01-20 17:12:54|         228|          26|   NULL| 2021-01-20|  2021-01-20|  s/b44|\n",
      "|           HV0003|              B02866|2021-01-29 17:27:39|2021-01-29 18:03:28|         123|         130|   NULL| 2021-01-29|  2021-01-29|  e/b32|\n",
      "|           HV0003|              B02878|2021-01-16 14:47:42|2021-01-16 15:06:44|          47|         119|   NULL| 2021-01-16|  2021-01-16|  e/b3e|\n",
      "|           HV0003|              B02866|2021-01-03 05:33:48|2021-01-03 06:04:01|          14|          75|   NULL| 2021-01-03|  2021-01-03|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-28 19:54:37|2021-01-28 20:01:44|          61|          61|   NULL| 2021-01-28|  2021-01-28|  e/9ce|\n",
      "|           HV0003|              B02875|2021-01-10 16:31:19|2021-01-10 16:36:52|         150|         150|   NULL| 2021-01-10|  2021-01-10|  e/b3b|\n",
      "|           HV0003|              B02764|2021-01-30 17:44:41|2021-01-30 17:52:11|         205|         205|   NULL| 2021-01-30|  2021-01-30|  e/acc|\n",
      "|           HV0003|              B02765|2021-01-15 15:15:26|2021-01-15 15:40:27|         129|         145|   NULL| 2021-01-15|  2021-01-15|  s/acd|\n",
      "|           HV0003|              B02887|2021-01-14 09:18:22|2021-01-14 09:45:49|         157|         236|   NULL| 2021-01-14|  2021-01-14|  e/b47|\n",
      "|           HV0005|              B02510|2021-01-22 21:46:14|2021-01-22 22:04:37|          76|          91|   NULL| 2021-01-22|  2021-01-22|  e/9ce|\n",
      "|           HV0003|              B02872|2021-01-22 15:52:23|2021-01-22 16:01:16|          41|          42|   NULL| 2021-01-22|  2021-01-22|  e/b38|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_fhvhv\n",
    " .withColumn('pickup_date', F.to_date(df_fhvhv.pickup_datetime))\n",
    " .withColumn('dropoff_date', F.to_date(df_fhvhv.dropoff_datetime)) \n",
    " .withColumn('base_id', crazy_stuff_udf(df_fhvhv.dispatching_base_num))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read some yellow csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 07:10:52 WARN SparkContext: The path https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-06.csv.gz has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "yellow_file_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-06.csv.gz'\n",
    "spark.sparkContext.addFile(yellow_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read.csv(SparkFiles.get('yellow_tripdata_2019-06.csv.gz'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow.withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.createOrReplaceTempView('yellow_trips_june_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_file_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-06.csv.gz'\n",
    "spark.sparkContext.addFile(green_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.csv(SparkFiles.get('green_tripdata_2019-06.csv.gz'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumn('service_type', F.lit('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.createOrReplaceTempView('green_trips_june_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|      yellow| 6941024|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    service_type,\n",
    "    count(1)\n",
    "from yellow_trips_june_2019\n",
    "group by service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow_trips_june_2019\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green_trips_june_2019\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue.join(df_yellow_revenue, on=['hour','zone'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+--------------+-----------------+--------------+\n",
      "|               hour|zone|amount|number_records|           amount|number_records|\n",
      "+-------------------+----+------+--------------+-----------------+--------------+\n",
      "|2008-12-31 23:00:00|  93|  NULL|          NULL|            61.42|             1|\n",
      "|2008-12-31 23:00:00| 170|  NULL|          NULL|70.27000000000001|             3|\n",
      "|2009-01-01 00:00:00|  90|  NULL|          NULL|             11.3|             1|\n",
      "|2009-01-01 00:00:00| 138|  NULL|          NULL|             35.9|             1|\n",
      "|2009-01-01 00:00:00| 238|  NULL|          NULL|             30.6|             2|\n",
      "|2009-01-01 08:00:00| 231|  NULL|          NULL|             42.8|             1|\n",
      "|2009-01-01 11:00:00| 237|  NULL|          NULL|              8.3|             1|\n",
      "|2009-01-01 14:00:00|  68|  NULL|          NULL|              9.8|             1|\n",
      "|2019-05-30 14:00:00|  79|  NULL|          NULL|            11.16|             1|\n",
      "|2019-05-30 14:00:00| 107|  NULL|          NULL|             10.0|             1|\n",
      "|2019-05-30 22:00:00| 164|  NULL|          NULL|             8.03|             1|\n",
      "|2019-05-31 04:00:00|  90|  NULL|          NULL|             11.3|             1|\n",
      "|2019-05-31 07:00:00| 237|  NULL|          NULL|              9.3|             1|\n",
      "|2019-05-31 09:00:00|  48|  NULL|          NULL|            17.76|             1|\n",
      "|2019-05-31 10:00:00|  68|  NULL|          NULL|             9.96|             1|\n",
      "|2019-05-31 10:00:00| 236|  NULL|          NULL|             11.8|             1|\n",
      "|2019-05-31 10:00:00| 263|  NULL|          NULL|            11.16|             1|\n",
      "|2019-05-31 11:00:00|  90|  NULL|          NULL|             11.8|             1|\n",
      "|2019-05-31 11:00:00| 246|  NULL|          NULL|            12.96|             1|\n",
      "|2019-05-31 12:00:00|  50|  NULL|          NULL|            50.52|             1|\n",
      "+-------------------+----+------+--------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join with zones dataframe to get information on zone names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: int, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_join.join(df_zones, df_join.zone == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+--------------+-----------------+--------------+---------+--------------------+------------+\n",
      "|               hour|zone|amount|number_records|           amount|number_records|  Borough|                Zone|service_zone|\n",
      "+-------------------+----+------+--------------+-----------------+--------------+---------+--------------------+------------+\n",
      "|2008-12-31 23:00:00|  93|  NULL|          NULL|            61.42|             1|   Queens|Flushing Meadows-...|   Boro Zone|\n",
      "|2008-12-31 23:00:00| 170|  NULL|          NULL|70.27000000000001|             3|Manhattan|         Murray Hill| Yellow Zone|\n",
      "|2009-01-01 00:00:00|  90|  NULL|          NULL|             11.3|             1|Manhattan|            Flatiron| Yellow Zone|\n",
      "|2009-01-01 00:00:00| 138|  NULL|          NULL|             35.9|             1|   Queens|   LaGuardia Airport|    Airports|\n",
      "|2009-01-01 00:00:00| 238|  NULL|          NULL|             30.6|             2|Manhattan|Upper West Side N...| Yellow Zone|\n",
      "|2009-01-01 08:00:00| 231|  NULL|          NULL|             42.8|             1|Manhattan|TriBeCa/Civic Center| Yellow Zone|\n",
      "|2009-01-01 11:00:00| 237|  NULL|          NULL|              8.3|             1|Manhattan|Upper East Side S...| Yellow Zone|\n",
      "|2009-01-01 14:00:00|  68|  NULL|          NULL|              9.8|             1|Manhattan|        East Chelsea| Yellow Zone|\n",
      "|2019-05-30 14:00:00|  79|  NULL|          NULL|            11.16|             1|Manhattan|        East Village| Yellow Zone|\n",
      "|2019-05-30 14:00:00| 107|  NULL|          NULL|             10.0|             1|Manhattan|            Gramercy| Yellow Zone|\n",
      "|2019-05-30 22:00:00| 164|  NULL|          NULL|             8.03|             1|Manhattan|       Midtown South| Yellow Zone|\n",
      "|2019-05-31 04:00:00|  90|  NULL|          NULL|             11.3|             1|Manhattan|            Flatiron| Yellow Zone|\n",
      "|2019-05-31 07:00:00| 237|  NULL|          NULL|              9.3|             1|Manhattan|Upper East Side S...| Yellow Zone|\n",
      "|2019-05-31 09:00:00|  48|  NULL|          NULL|            17.76|             1|Manhattan|        Clinton East| Yellow Zone|\n",
      "|2019-05-31 10:00:00|  68|  NULL|          NULL|             9.96|             1|Manhattan|        East Chelsea| Yellow Zone|\n",
      "|2019-05-31 10:00:00| 236|  NULL|          NULL|             11.8|             1|Manhattan|Upper East Side N...| Yellow Zone|\n",
      "|2019-05-31 10:00:00| 263|  NULL|          NULL|            11.16|             1|Manhattan|      Yorkville West| Yellow Zone|\n",
      "|2019-05-31 11:00:00|  90|  NULL|          NULL|             11.8|             1|Manhattan|            Flatiron| Yellow Zone|\n",
      "|2019-05-31 11:00:00| 246|  NULL|          NULL|            12.96|             1|Manhattan|West Chelsea/Huds...| Yellow Zone|\n",
      "|2019-05-31 12:00:00|  50|  NULL|          NULL|            50.52|             1|Manhattan|        Clinton West| Yellow Zone|\n",
      "+-------------------+----+------+--------------+-----------------+--------------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_res.drop('LocationID', 'zone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
