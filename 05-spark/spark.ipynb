{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/29 15:54:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[4]\")\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-29 15:54:07--  http://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.133.88, 52.216.48.112, 52.216.170.221, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.133.88|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.1’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2024-02-29 15:54:07 (4.28 MB/s) - ‘taxi+_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"taxi+_zone_lookup.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('zones', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read FHVHV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/29 16:03:36 WARN SparkContext: The path https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz has been added already. Overwriting of added paths is not supported in the current version.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "file_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz'\n",
    "spark.sparkContext.addFile(file_url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get('fhvhv_tripdata_2021-01.csv.gz'), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer schema by reading first 100 rows into pandas and then converting to spark making small adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first 100 rows using Pandas\n",
    "first_100_rows = pd.read_csv(file_url, compression='gzip', nrows=100, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "SR_Flag                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_100_rows.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_100_rows = pd.read_csv(file_url, compression='gzip', nrows=100, parse_dates=['pickup_datetime', 'dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_100_rows_sp = spark.createDataFrame(first_100_rows)\n",
    "first_100_rows_sp = (first_100_rows_sp\n",
    "                     .withColumn(\"SR_Flag\", first_100_rows_sp[\"SR_Flag\"].cast(IntegerType()))\n",
    "                     .withColumn(\"DOLocationID\", first_100_rows_sp[\"DOLocationID\"].cast(IntegerType()))\n",
    "                     .withColumn(\"PULocationID\", first_100_rows_sp[\"PULocationID\"].cast(IntegerType())))\n",
    "first_100_rows_sp.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read FHVHV data again providing the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(SparkFiles.get('fhvhv_tripdata_2021-01.csv.gz'), header=True, schema=first_100_rows_sp.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the obtained csv to partitioned parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(24).write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that resulting parquet files are considerably smaller than the original csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 461272\n",
      "-rw-r--r--@ 1 alexlitvinov  staff     0B Feb 29 20:23 _SUCCESS\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00000-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00001-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00002-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00003-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00004-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00005-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00006-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00007-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00008-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00009-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00010-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00011-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00012-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00013-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00014-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00015-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00016-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00017-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00018-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00019-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00020-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00021-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00022-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 alexlitvinov  staff   9.4M Feb 29 20:23 part-00023-c61329de-c6e9-4b09-83e0-872bb659fe6f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/01/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read repartitioned df back into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: int]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-12 09:46:38|2021-01-12 09:59:31|           4|         137|\n",
      "|2021-01-22 18:07:54|2021-01-22 18:13:25|         242|          78|\n",
      "|2021-01-19 08:55:37|2021-01-19 09:09:36|         244|         128|\n",
      "|2021-01-12 04:16:21|2021-01-12 04:37:52|         230|         225|\n",
      "|2021-01-20 17:00:35|2021-01-20 17:12:54|         228|          26|\n",
      "|2021-01-29 17:27:39|2021-01-29 18:03:28|         123|         130|\n",
      "|2021-01-16 14:47:42|2021-01-16 15:06:44|          47|         119|\n",
      "|2021-01-03 05:33:48|2021-01-03 06:04:01|          14|          75|\n",
      "|2021-01-10 16:31:19|2021-01-10 16:36:52|         150|         150|\n",
      "|2021-01-30 17:44:41|2021-01-30 17:52:11|         205|         205|\n",
      "|2021-01-15 15:15:26|2021-01-15 15:40:27|         129|         145|\n",
      "|2021-01-14 09:18:22|2021-01-14 09:45:49|         157|         236|\n",
      "|2021-01-22 15:52:23|2021-01-22 16:01:16|          41|          42|\n",
      "|2021-01-29 04:39:53|2021-01-29 04:49:37|         213|         250|\n",
      "|2021-01-31 02:04:49|2021-01-31 02:25:57|         167|         243|\n",
      "|2021-01-11 20:59:47|2021-01-11 21:12:33|         164|         226|\n",
      "|2021-01-05 16:20:14|2021-01-05 16:29:35|         239|         238|\n",
      "|2021-01-13 12:25:56|2021-01-13 12:28:41|          42|          42|\n",
      "|2021-01-12 22:14:28|2021-01-12 22:26:10|         197|         135|\n",
      "|2021-01-29 14:46:57|2021-01-29 15:14:45|          79|          74|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\n",
    " .filter(df.hvfhs_license_num == 'HV0003')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark in-built sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|base_id|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-25 18:08:34|2021-01-25 18:20:34|         247|         169|   NULL| 2021-01-25|  2021-01-25|  e/9ce|\n",
      "|           HV0005|              B02510|2021-01-01 06:52:41|2021-01-01 06:59:34|          14|         227|   NULL| 2021-01-01|  2021-01-01|  e/9ce|\n",
      "|           HV0005|              B02510|2021-01-29 10:47:17|2021-01-29 11:06:12|         134|         138|   NULL| 2021-01-29|  2021-01-29|  e/9ce|\n",
      "|           HV0003|              B02866|2021-01-12 09:46:38|2021-01-12 09:59:31|           4|         137|   NULL| 2021-01-12|  2021-01-12|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-26 22:19:36|2021-01-26 22:43:02|           7|         239|   NULL| 2021-01-26|  2021-01-26|  e/9ce|\n",
      "|           HV0003|              B02866|2021-01-22 18:07:54|2021-01-22 18:13:25|         242|          78|   NULL| 2021-01-22|  2021-01-22|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-23 18:18:33|2021-01-23 18:29:34|          80|         148|   NULL| 2021-01-23|  2021-01-23|  e/9ce|\n",
      "|           HV0003|              B02764|2021-01-19 08:55:37|2021-01-19 09:09:36|         244|         128|   NULL| 2021-01-19|  2021-01-19|  e/acc|\n",
      "|           HV0003|              B02872|2021-01-12 04:16:21|2021-01-12 04:37:52|         230|         225|   NULL| 2021-01-12|  2021-01-12|  e/b38|\n",
      "|           HV0003|              B02884|2021-01-20 17:00:35|2021-01-20 17:12:54|         228|          26|   NULL| 2021-01-20|  2021-01-20|  s/b44|\n",
      "|           HV0003|              B02866|2021-01-29 17:27:39|2021-01-29 18:03:28|         123|         130|   NULL| 2021-01-29|  2021-01-29|  e/b32|\n",
      "|           HV0003|              B02878|2021-01-16 14:47:42|2021-01-16 15:06:44|          47|         119|   NULL| 2021-01-16|  2021-01-16|  e/b3e|\n",
      "|           HV0003|              B02866|2021-01-03 05:33:48|2021-01-03 06:04:01|          14|          75|   NULL| 2021-01-03|  2021-01-03|  e/b32|\n",
      "|           HV0005|              B02510|2021-01-28 19:54:37|2021-01-28 20:01:44|          61|          61|   NULL| 2021-01-28|  2021-01-28|  e/9ce|\n",
      "|           HV0003|              B02875|2021-01-10 16:31:19|2021-01-10 16:36:52|         150|         150|   NULL| 2021-01-10|  2021-01-10|  e/b3b|\n",
      "|           HV0003|              B02764|2021-01-30 17:44:41|2021-01-30 17:52:11|         205|         205|   NULL| 2021-01-30|  2021-01-30|  e/acc|\n",
      "|           HV0003|              B02765|2021-01-15 15:15:26|2021-01-15 15:40:27|         129|         145|   NULL| 2021-01-15|  2021-01-15|  s/acd|\n",
      "|           HV0003|              B02887|2021-01-14 09:18:22|2021-01-14 09:45:49|         157|         236|   NULL| 2021-01-14|  2021-01-14|  e/b47|\n",
      "|           HV0005|              B02510|2021-01-22 21:46:14|2021-01-22 22:04:37|          76|          91|   NULL| 2021-01-22|  2021-01-22|  e/9ce|\n",
      "|           HV0003|              B02872|2021-01-22 15:52:23|2021-01-22 16:01:16|          41|          42|   NULL| 2021-01-22|  2021-01-22|  e/b38|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .withColumn('pickup_date', F.to_date(df.pickup_datetime))\n",
    " .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \n",
    " .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read some yellow csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_file_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-06.csv.gz'\n",
    "spark.sparkContext.addFile(yellow_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read.csv(SparkFiles.get('yellow_tripdata_2019-06.csv.gz'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow.withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.createOrReplaceTempView('yellow_trips_june_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|      yellow| 6941024|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    service_type,\n",
    "    count(1)\n",
    "from yellow_trips_june_2019\n",
    "group by service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: int, trip_distance: double, RatecodeID: int, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, service_type: string]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df_res = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow_trips_june_2019\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
